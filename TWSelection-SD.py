import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn import preprocessing
from collections import defaultdict, Counter

from statsmodels.graphics.tsaplots import plot_pacf, plot_acf
from statsmodels.tsa.arima_model import ARIMA
import csv
import re

class TWSelection:

    def TW_discovery_process_calculation_twlist(self, event_log, tw_lists, aspect):
        Overall_dict = {}
        Arrival_rate_dict = {}
        event_log['Complete Timestamp'] = pd.to_datetime(event_log['Complete Timestamp'], errors='coerce')
        event_log['Start Timestamp'] = pd.to_datetime(event_log['Start Timestamp'], errors='coerce')
        event_log['Activity Duration'] = abs(event_log['Complete Timestamp'] - event_log['Start Timestamp'])
        start_unit_log = min(event_log['Start Timestamp'])
        end_log = max(event_log['Complete Timestamp'])
        arr_temp_event_log = event_log.sort_values(['Start Timestamp'], ascending=True).groupby(['Case ID'])
        fin_temp_event_log = event_log.sort_values(['Start Timestamp'], ascending=True).groupby(['Case ID'])
        case_dur_temp_log = event_log.groupby(['Case ID'])
        Name_General_selected_variables_dict = []
        for tw_list in tw_lists:
            # todo Arrival Rate of Cases per Day, Week, Month
            y = int((re.findall(r'\d+', tw_list))[0])
            z = tw_list[-1]
    
            if z == "H" and (24 % y == 0) or z != 'H':
    
                startdiff = []
                startforeach = []
                for actname, actgroup in arr_temp_event_log:
                    startforeach.append(actgroup['Start Timestamp'].values[0])
                startforeach.sort()
    
                for i in range(len(startforeach) - 1):
                    startdiff.append((startforeach[i + 1] - startforeach[i]))
    
                count_sta = Counter(startforeach)
                df_startforeach = pd.Series(count_sta).to_frame()
                Hourly = df_startforeach.resample(str(tw_list)).sum()
                Hourly.columns = ['hourly']
    
                # todo Finish Rate of Cases per Day, Week, Month
                enddiff = []
                endforeach = []
                for actname, actgroup in fin_temp_event_log:
                    endforeach.append(actgroup['Complete Timestamp'].values[-1])
                endforeach.sort()
    
                for i in range(len(endforeach) - 1):
                    enddiff.append((endforeach[i + 1] - endforeach[i]))
    
                enddiff = pd.to_timedelta(enddiff).seconds / 3600
                counter_intervals = Counter(enddiff)
                ecount_sta = Counter(endforeach)
                edf_startforeach = pd.Series(ecount_sta).to_frame()
                eHourly = edf_startforeach.resample(str(tw_list)).sum()
    
                eHourly.columns = ['ehourly']
    
                # Todo Compute the number of in process cases (MAX Capacity)
                Hourly_df = pd.concat([Hourly, eHourly], axis=1, join='outer')
                Hourly_df.fillna(0, inplace=True)
    
                h_case_in_process = (Hourly_df['hourly'] - Hourly_df['ehourly'])
                temp_list_inproc = h_case_in_process.tolist()
                for i in range(len(temp_list_inproc)):
                    if i == 0:
                        temp_list_inproc[i] = h_case_in_process.tolist()[i]
                    else:
                        temp_list_inproc[i] = temp_list_inproc[i] + temp_list_inproc[i - 1]
    
                max_h_case_in_process = max(h_case_in_process)
    
            # TODO Process whole active Time and Process active time per W,D,M
            try:
                process_active_time = event_log['Activity Duration'].sum()
            except ValueError as err:
                print(err)
    
            temp_active_time = event_log[['Start Timestamp', 'Activity Duration']]
            temp_active_time['Start Timestamp'] = pd.to_datetime(temp_active_time['Start Timestamp'])
            sort_start_duration = temp_active_time.sort_values('Start Timestamp')
            sort_start_duration.set_index('Start Timestamp', inplace=True)
            process_H_active_time_df = sort_start_duration.resample(str(tw_list)).sum()
            process_H_active_time = process_H_active_time_df['Activity Duration'].values
            process_H_active_time = pd.to_timedelta(process_H_active_time).total_seconds() / 3600
            # TODO List of time in process per case and real time service (Case Durations)
            case_duration_list = []
            case_real_duration_list = []
            case_real_duration_list_waiting = []
            case_duration_list_waiting = []
            case_duration_dict = {}
            case_real_duration_dict = {}
            for dcase, dgroup in case_dur_temp_log:
                case_real_duration_list_waiting.append(np.sum(dgroup['Activity Duration']))
                case_duration_list_waiting.append(
                    np.max(dgroup['Complete Timestamp']) - np.min(dgroup['Start Timestamp']))
                case_duration_dict[np.min(dgroup['Start Timestamp'])] = pd.to_timedelta(
                    np.max(dgroup['Complete Timestamp']) - np.min(dgroup['Start Timestamp'])).total_seconds() / 3600
                case_real_duration_dict[np.min(dgroup['Start Timestamp'])] = pd.to_timedelta(
                    np.sum(dgroup['Activity Duration'])).total_seconds() / 3600
    
            # case_duration_dict = {k: v for k, v in case_duration_dict.items() if v>0.5}
            # case_duration_dict = {k: v for k, v in case_duration_dict.items() if np.abs(v - np.mean(list(case_duration_dict.values())))<= (np.std(list(case_duration_dict.values())))}
    
            # case_real_duration_dict = {k: v for k, v in case_real_duration_dict.items() if v > 0.5}
            # case_real_duration_dict = {k: v for k, v in case_real_duration_dict.items() if np.abs(v - np.mean(list(case_real_duration_dict.values()))) <=(np.std(list(case_real_duration_dict.values())))}
    
            case_duration_list = list(case_duration_dict.values())
            case_real_duration_list = list(case_real_duration_dict.values())
    
            case_duration_df = pd.DataFrame(case_duration_dict.items(), columns=['Start Timestamp', 'Case Duration'])
            case_duration_df = case_duration_df.sort_values('Start Timestamp')
            case_duration_df.set_index('Start Timestamp', inplace=True)
            case_duration_H_df = case_duration_df.resample(str(tw_list)).sum()
            case_duration_H_df.insert(0, 'Avg Case Duration', case_duration_df.resample(str(tw_list)).mean())
    
            case_real_duration_df = pd.DataFrame(case_real_duration_dict.items(),
                                                 columns=['Start Timestamp', 'Case Duration'])
            case_real_duration_df = case_real_duration_df.sort_values('Start Timestamp')
            case_real_duration_df.set_index('Start Timestamp', inplace=True)
            case_real_duration_H_df = case_real_duration_df.resample(str(tw_list)).sum()
            case_real_duration_H_df.insert(0, 'Avg Case Duration', case_real_duration_df.resample(str(tw_list)).mean())
            case_real_duration_H_df.fillna(0)
    
            case_duration_list_waiting = pd.to_timedelta(case_duration_list_waiting).total_seconds() / 3600
            case_real_duration_list_waiting = pd.to_timedelta(case_real_duration_list_waiting).total_seconds() / 3600
            waiting_time = np.array(case_duration_list_waiting) - np.array(case_real_duration_list_waiting)
    
            case_duration_coutner = Counter(case_duration_list)
            case_real_duration_coutner = Counter(case_real_duration_list)
            avg_case_duration = np.mean(case_duration_list)
            avg_case_real_duration = np.mean(case_real_duration_list)
    
            # TODO Number of Resources and unique resources per
            temp_event_log_start_index = event_log.set_index('Start Timestamp')
            temp_group_h = temp_event_log_start_index.groupby(pd.Grouper(freq=str(tw_list)))
    
            num_unique_resource_h = (temp_group_h['Resource'].nunique()).values
    
            """
            finsihed_week =[]
            for sk in sklist:
                finish_counter = 0
                tem_com = temp_group_w.get(sk)
                for i in tem_com['Complete Timestamp']:
                    if sk>=i:
                        finish_counter += 1
                finsihed_week.append(finish_counter)
                """
    
            # TODO Number of Resources and unique resources per case
            uniqe_resource_list_per_case = []
            resource_list_per_case = []
            for rc, rgroup in case_dur_temp_log:
                resource_per_case = rgroup['Resource']
                unique_resource_per_case = np.unique(resource_per_case)
                uniqe_resource_list_per_case.append(len(unique_resource_per_case))
                resource_list_per_case.append(len(resource_per_case))
    
            # TODO Create Overall Dict
    
            Arrival_rate_dict[str(tw_list)] = Hourly['hourly'].values.tolist()
    
            Overall_dict["Arrival rate"] = Arrival_rate_dict
            Name_General_selected_variables_dict.append(str(aspect) + "_" + str(tw_list) + "_sdlog.csv")
            General_selected_variables_dict = {"Arrival rate" + str(tw_list): Hourly['hourly'].values.tolist(),
                                               "Finish rate" + str(tw_list): (eHourly['ehourly'].values).tolist(),
                                               "Num of unique resource" + str(tw_list): num_unique_resource_h.tolist(),
                                               "Process active time" + str(tw_list): case_duration_H_df[
                                                   'Case Duration'].tolist(),
                                               "Service time per case" + str(tw_list): case_real_duration_H_df[
                                                   'Avg Case Duration'].tolist(),
                                               "Time in process per case" + str(tw_list): case_duration_H_df[
                                                   'Avg Case Duration'].tolist(),
                                               "Waiting time in process per case" + str(tw_list): pd.array(
                                                   case_duration_H_df['Avg Case Duration'].tolist()) - pd.array(
                                                   case_real_duration_H_df['Avg Case Duration'].tolist()),
                                               "Num in process case" + str(tw_list): temp_list_inproc,
                                               }
            with open(str(aspect) + "_" + str(tw_list) + "_sdlog.csv", 'w') as f:
                writer = csv.writer(f)
                writer.writerow(General_selected_variables_dict.keys())
                x = zip(*General_selected_variables_dict.values())
                xx = zip(*x)
                xxx = zip(*xx)
                writer.writerows(xxx)
    
        return Overall_dict, Name_General_selected_variables_dict
    def Detect_pattern_tw(self, Overall_dict, event_log):
            # TODO  Find suitable  bin  for time
            # duration of log in H, D,W,M:
            """
            end_log = pd.to_datetime(np.max(event_log["Complete Timestamp"].values))
            start_log = pd.to_datetime(np.min(event_log["Start Timestamp"].values))
            duration_log = end_log - start_log
            duration_log_H = duration_log.total_seconds() / 3600
            duration_log_D = duration_log.days
            duration_log_W = (duration_log.total_seconds() / 86400) // 7
            duration_log_M = ((duration_log.total_seconds() / 86400) // 7) // 4
            # number of instance (case, events,...):
            instance_num = event_log["Case ID"].nunique()
            # plot each differnt bin:
            bin_H = instance_num / duration_log_H
            bin_D = instance_num / duration_log_D
            bin_W = instance_num / duration_log_W
            # bin_M = instance_num/duration_log_M
            # plt.hist()
            """
            # Todo Read features from SD: dict
    
            TW_Dete_dict = defaultdict(list)
            number_figure = round(math.sqrt(len(Overall_dict["Arrival rate"].keys())))
    
            vid = 1
            plt.figure()
            for ktw, vtw in Overall_dict["Arrival rate"].items():
                plt.rcParams["figure.figsize"] = [10, 10]
                plt.subplot(2, 2, vid)
                plt.tight_layout()
                plt.gca().set_title("Arrival Rate Per " + str(ktw), size=8)
    

                vtwn = (vtw - np.min(vtw)) / (np.max(vtw) - np.min(vtw))
                plt.bar([i for i in range(0, len(vtwn))], vtwn, label=str(ktw))
                vid = vid + 1
    
            #plt.savefig('/UserPattern.png', dpi=100)
            plt.show()
                #plt.bar([i for i in range(0, len(vtw))], vtw)
                # plt.show()
    
                # TODO Detrend by removing difference:
               # diff = list()
                #for i in range(1, len(vtw)):
                 #   value = vtw[i] - vtw[i - 1]
                  #  diff.append(value)
                # plt.plot(diff)
                # plt.show()
    
                # ،TODO Find lag with maximum Corr and best TW:
            for ktw, vtw in Overall_dict["Arrival rate"].items():
                max_lag = (len(vtw)//10)
                if max_lag >0:
    
                    plt.figure()
                    temp_acorr = plt.acorr(np.array(vtw).astype(float), maxlags=max_lag)
                    temp_acorr_1 = plt.acorr(np.array(vtw).astype(float), maxlags=max_lag)
                    # temp_acorr = plt.acorr(np.array(vtw).astype(float))
                    temp_acorr[1][temp_acorr[1] == 1.0] = 0
                    TW_Dete_dict[ktw].append(np.max(temp_acorr[1]))
                    index_max = np.argmax(temp_acorr[1])
                    TW_Dete_dict[ktw].append(temp_acorr[0][index_max])
                    TW_Dete_dict[ktw].append(temp_acorr[1][np.where(temp_acorr[0]==1)])
                else:
    
                    TW_Dete_dict[ktw].append(0)
                    TW_Dete_dict[ktw].append(0)
                    TW_Dete_dict[ktw].append(0)
                # TODO find lag using CAF and PCAF
                plot_pacf(vtw, lags=7)
                plot_acf(vtw, lags=7)
    
    
            return TW_Dete_dict
    def Post_process_tw(self,SD_Log, TW_Dete_dict):
    
            #SD_Log = pd.read_csv("General2H_sdlog.csv")
            SD_Log = SD_Log.fillna(0)
            c = str(SD_Log.columns[0])[-2:]
            temp_pattern = abs(TW_Dete_dict.get(c)[1])
            target_feature_values = SD_Log[SD_Log.columns[0]]
            new_df = SD_Log.loc[target_feature_values == 0]
            inactive_array = (new_df == 0).astype(int).sum(axis=1) / len(new_df.columns)
            my_list = list(inactive_array[inactive_array < 0.9].index)
            newnewdf = new_df.loc[my_list]
            ind = new_df.index
            Active_SD_Log = SD_Log.drop(SD_Log.index[ind])
            diff = list()
            for i in range(1, len(ind)):
                value = ind[i] - ind[i - 1]
                diff.append(value)
            # plt.bar([x for x in range(0,len(diff))],diff)
            plt.hist(diff)
            Active_SD_Log.to_csv(r"Active" + "_" + str(c) + "_sdlog.csv",index=False)
            return Active_SD_Log
    def Detect_best_user_tw(self, TW_Dete_dict, Overall_dict):
    
            error_delta_dict = {}
            d = {}
            for k, v in Overall_dict.get("Arrival rate").items():
                v = list(filter(lambda a: a != 0, v))
                best_lag = abs(TW_Dete_dict.get(k)[1])
                diff = list()
                for i in range(1, len(v)):
                    value = v[i] - v[i - 1]
                    diff.append(value)
                if len(diff) > 5 :
                    if 0<best_lag and best_lag <len(diff)//5 and best_lag< 5 :
                        arima_model = ARIMA(v, order=(best_lag, 0, 1))
                        model_fit = arima_model.fit(start_ar_lags=best_lag)
                        t = model_fit.predict()
                        error = np.mean(abs(v - t) / v) *100
                        error_delta_dict[str(k)] = error
    
                    else:
    
                        arima_model = ARIMA(v, order=(1, 0, 1))
                        model_fit = arima_model.fit(start_ar_lags=2)
                        t = model_fit.predict()
                        error = np.mean(abs(v - t) / (v))*100
                        error_delta_dict[str(k)] = error
                else:
                    error_delta_dict[str(k)] = -0.1
    
            lists = sorted(error_delta_dict.items())  # sorted by key, return a list of tuples
            x, y = zip(*lists)
            plt.figure()
            plt.plot(x, y, '--bo')
            plt.title("Error of Different Time Delta")
            plt.xlabel("Time Delta")
            plt.ylabel("PAME")
            #plt.savefig('/UserTWError.png', dpi=100)
            plt.show()
            return

if __name__=="__main__":
    tws = TWSelection()
    tw_result=""
    event_log= "event_log.csv"
    event_log = pd.read_csv(event_log)
    tw_lists = ['1H','7D','1W'] # sample time window lists = [8H,2D,1W,1M]
    tw_discovered = tws.TW_discovery_process_calculation_twlist(event_log=event_log, tw_lists=tw_lists, aspect="General")
    generated_SD_log = tw_discovered[1]
    overall_dict = tw_discovered[0]
    TW_Dete_dict = tws.Detect_pattern_tw(overall_dict, event_log)
    tw_best = max(TW_Dete_dict, key=TW_Dete_dict.get)
    tw_worse = min(TW_Dete_dict, key=TW_Dete_dict.get)
    for k, v in TW_Dete_dict.items():
        if abs(v[2]) > 0.5:
            tw_result = tw_result + ' ' + str(k) + ' is a strong pattern.'
        else:
            tw_result = tw_result + ' ' + str(k) + ' is not a strong pattern.'

    tw_result = tw_result + "\n The Strongest Pattern Discovered: " + str(
        abs(TW_Dete_dict.get(tw_best)[1])) + " " + str(tw_best) + "\n"
    # "\n The Weakest Pattern: "+str(abs(TW_Dete_dict.get(tw_worse)[1]))+' For ' +str(tw_worse)

    active_overall_dict = {}
    active_overall_dict["Arrival rate"] = {}
    for sdlog_name in generated_SD_log:
        sd_log = pd.read_csv(sdlog_name)
        name_sd_tw = sdlog_name.split("_")[1]
        overall_dict["Arrival rate"].get(name_sd_tw)
        Active_SD_Log = (tws.Post_process_tw(sd_log, TW_Dete_dict))
        active_overall_dict["Arrival rate"][name_sd_tw] = Active_SD_Log[Active_SD_Log.columns[0]]

        active_TW_Dete_dict = tws.Detect_pattern_tw(active_overall_dict, event_log)

    tws.Detect_best_user_tw(active_TW_Dete_dict, active_overall_dict)